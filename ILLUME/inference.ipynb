{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "447d2b3e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# ILLUME+ Model Inference\n",
    "\n",
    "This notebook demonstrates how to use the ILLUME model for three main tasks:\n",
    "1. **Image Understanding:** Given an image and a text prompt (e.g., a question), the model generates a textual response.\n",
    "2. **Image Generation:** Given a text prompt, the model generates an image.\n",
    "3. **Image Editing:** Given a text prompt with source image, the model generates the edited image.\n",
    "\n",
    "**Important:** You should download the models and place the model in the same folder as described in Section 1. Otherwise you need to modify the model path in Section 3."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecf12201",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 1. Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fee59f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  If you haven't download the checkpoint, you could uncomment this to download.\n",
    "\n",
    "# from huggingface_hub import snapshot_download\n",
    "# import os\n",
    "\n",
    "\n",
    "# if not os.path.exists('../checkpoints/'):\n",
    "#     os.makedirs('../checkpoints/')\n",
    "\n",
    "# save_dir=\"../checkpoints/illume_plus-qwen2_5-3b\"\n",
    "# snapshot_download(local_dir=save_dir, \n",
    "#                   repo_id='ILLUME-MLLM/illume_plus-qwen2_5-3b', \n",
    "#                   local_dir_use_symlinks=False, \n",
    "#                   resume_download=True))\"\n",
    "# os.makedirs('./logdir/illume_plus_3b/', exist_ok=True)\n",
    "# os.symlink(os.path.abspath(save_dir), os.path.abspath('./logdir/illume_plus_3b/illume_plus-qwen2_5-3b_stage3'))\n",
    "\n",
    "# DUALVITOK_CHECKPOINT_DIR=\"../checkpoints/dualvitok/\"\n",
    "# snapshot_download(local_dir=DUALVITOK_CHECKPOINT_DIR, \n",
    "#                   repo_id='ILLUME-MLLM/dualvitok', \n",
    "#                   local_dir_use_symlinks=False, \n",
    "#                   resume_download=True))\"\n",
    "\n",
    "# DUALVITOK_SDXL_DECODER_DIR=\"../checkpoints/dualvitok-sdxl-decoder/\"\n",
    "# snapshot_download(local_dir=DUALVITOK_SDXL_DECODER_DIR, \n",
    "#                   repo_id='ILLUME-MLLM/dualvitok-sdxl-decoder', \n",
    "#                   local_dir_use_symlinks=False, \n",
    "#                   resume_download=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3ca5bf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# link the download path to the checkpoint path designed in the mllm config.\n",
    "import os\n",
    "\n",
    "os.makedirs('./logdir/illume_plus_3b/', exist_ok=True)\n",
    "if not os.path.exists('./logdir/illume_plus_3b/illume_plus-qwen2_5-3b_stage3/'):\n",
    "    os.symlink(os.path.abspath('../checkpoints/illume_plus-qwen2_5-3b'),\n",
    "                          os.path.abspath('./logdir/illume_plus_3b/illume_plus-qwen2_5-3b_stage3/'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49d7f8a2",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.abspath(os.path.join(os.path.abspath('.'), '../vision_tokenizer/')))\n",
    "\n",
    "import argparse\n",
    "import traceback\n",
    "import logging\n",
    "from functools import partial\n",
    "import numpy as np\n",
    "\n",
    "from PIL import Image\n",
    "import re  # Added for parsing image tokens\n",
    "from typing import List, Tuple\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# --- Add necessary imports from your ILLUME codebase ---\n",
    "import torch\n",
    "\n",
    "from transformers import LogitsProcessorList, TextIteratorStreamer\n",
    "\n",
    "from generation_eval.models.builder import build_eval_model\n",
    "\n",
    "from illume.constants import IMAGE_TOKEN_INDEX, DEFAULT_IMAGE_TOKEN\n",
    "from illume.conversation import conv_templates, default_conversation  # Import Conversation class\n",
    "from illume.mm_utils import process_images, tokenizer_image_token\n",
    "from illume.data.data_utils import unpad_and_resize_back\n",
    "\n",
    "# Logging setup\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686cffbf",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 2. Helper Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fbc4900",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def pad_sequence(tokenizer, input_ids_list: List[torch.Tensor], batch_first: bool, padding_value: int) -> torch.Tensor:\n",
    "    # Assuming input_ids_list contains tensors\n",
    "    # This is a simplified version. The app.py version handles left padding.\n",
    "    # For notebook usage with single items, direct padding or checking might be easier.\n",
    "    if tokenizer.padding_side == 'left':\n",
    "        # Flip for padding, then flip back. Requires all inputs to be actual tensors.\n",
    "        input_ids_list = [torch.flip(_input_ids, [0]) for _input_ids in input_ids_list]\n",
    "\n",
    "    # torch.nn.utils.rnn.pad_sequence expects a list of Tensors\n",
    "    input_ids_padded = torch.nn.utils.rnn.pad_sequence(input_ids_list, batch_first=batch_first, padding_value=padding_value)\n",
    "\n",
    "    if tokenizer.padding_side == 'left':\n",
    "        input_ids_padded = torch.flip(input_ids_padded, [1]) # Flip along the sequence dimension\n",
    "    return input_ids_padded\n",
    "\n",
    "\n",
    "def show_image(image, short_side=256):\n",
    "    if isinstance(image, str):\n",
    "        image = Image.open(image)\n",
    "    elif isinstance(image, np.ndarray):\n",
    "        image = Image.fromarray(image)\n",
    "\n",
    "    w, h = image.size\n",
    "    if w < h:\n",
    "        new_w = short_side\n",
    "        new_h = int(h * short_side / w)\n",
    "    else:\n",
    "        new_h = short_side\n",
    "        new_w = int(w * short_side / h)\n",
    "    image = image.resize((new_w, new_h))\n",
    "\n",
    "    plt.imshow(image)\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def convert_np_to_pil_img(samples, batch_data):\n",
    "    all_pil_images = []\n",
    "    for sample, info in zip(samples, batch_data):\n",
    "        img = Image.fromarray(sample.astype(np.uint8))\n",
    "\n",
    "        if \"original_sizes\" in info:  # for editing task, unpad and resize back to its original image size\n",
    "            original_size = info[\"original_sizes\"]\n",
    "            img = inference_engine.unpad_and_resize_back(img, original_size[0], original_size[1])\n",
    "        all_pil_images.append(img)\n",
    "    return all_pil_images\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38e2f956",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 3. Model Loading\n",
    "\n",
    "If your machine has more than 3 GPUs, the mllm, tokenizer and diffusion decoder will be placed in different GPUs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779b2536",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "model_name = 'ILLUME'\n",
    "\n",
    "mllm_config_path=\"../configs/example/illume_plus_3b/illume_plus_qwen2_5_3b_stage3.py\"\n",
    "tokenizer_config_path=\"../configs/example/dualvitok/dualvitok_anyres_max512.py\"\n",
    "vq_tokenizer_ckpt_path=\"../checkpoints/dualvitok/pytorch_model.bin\"\n",
    "diffusion_decoder_path=\"../checkpoints/dualvitok-sdxl-decoder/\"\n",
    "torch_dtype = 'fp16'\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "local_rank = 0 if 'cuda' in device else -1\n",
    "\n",
    "eval_model_cfg = dict(\n",
    "    type=model_name,\n",
    "    config=mllm_config_path,\n",
    "    tokenizer_config=tokenizer_config_path,\n",
    "    diffusion_decoder_path=diffusion_decoder_path,\n",
    "    tokenizer_checkpoint=vq_tokenizer_ckpt_path,\n",
    "    torch_dtype=torch_dtype\n",
    ")\n",
    "logging.info(f'Building ILLUME model with config: {eval_model_cfg}')\n",
    "inference_engine = build_eval_model(eval_model_cfg)\n",
    "\n",
    "# Device assignment\n",
    "num_gpus = torch.cuda.device_count()\n",
    "if num_gpus >= 3:\n",
    "    mllm_device = torch.device('cuda:0'); vq_device = torch.device('cuda:1'); diffusion_device = torch.device('cuda:2')\n",
    "elif num_gpus == 2:\n",
    "    mllm_device = torch.device('cuda:0'); vq_device = torch.device('cuda:1'); diffusion_device = torch.device('cuda:1')\n",
    "elif num_gpus == 1:\n",
    "    mllm_device = torch.device('cuda:0'); vq_device = torch.device('cuda:0'); diffusion_device = torch.device('cuda:0')\n",
    "else:\n",
    "    mllm_device = torch.device('cpu'); vq_device = torch.device('cpu'); diffusion_device = torch.device('cpu')\n",
    "logging.info(f'MLLM: {mllm_device}, VQ: {vq_device}, Diffusion: {diffusion_device}')\n",
    "\n",
    "if hasattr(inference_engine, 'mllm_model') and inference_engine.mllm_model: inference_engine.mllm_model.to(mllm_device)\n",
    "if hasattr(inference_engine, 'vq_model') and inference_engine.vq_model: inference_engine.vq_model.to(vq_device)\n",
    "if hasattr(inference_engine, 'diffusion_decoder_pipe') and inference_engine.diffusion_decoder_pipe: inference_engine.diffusion_decoder_pipe.to(diffusion_device)\n",
    "\n",
    "inference_engine.device = device # Overall device\n",
    "inference_engine.mllm_device = mllm_device\n",
    "inference_engine.vq_device = vq_device\n",
    "inference_engine.diffusion_device = diffusion_device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f806cc3",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 4. Image Understanding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a8739ed",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 0. Load Image\n",
    "prompt = 'depict the image in short'\n",
    "image_path = '../configs/data_configs/test_data_examples/ImageUnderstandingExample/images/0.png'\n",
    "input_image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "inference_config = inference_engine.prepare_inference_config(\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    top_p=1.0,\n",
    ")\n",
    "\n",
    "batch_data = [\n",
    "    dict(prompt=prompt, images_data=[input_image])\n",
    "]\n",
    "\n",
    "outputs = inference_engine.inference_mllm(\n",
    "    batch_data, inference_config,\n",
    "    is_img_gen_task=False,  #  Remember set this for image understanding. \n",
    "    do_sample=False  # You could add more params for the model.generate.\n",
    ")\n",
    "\n",
    "# outputs is a list. each element is a dict with keys: 'image_embed_inds', 'output_text', 'image_sizes', 'original_sizes'\n",
    "\n",
    "show_image(input_image)\n",
    "print(f\"Question: {prompt}\")\n",
    "print(f\"Answer:  {outputs[0]['output_text']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5870511e",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## 5. Image Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38612597",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# 0. Load Image\n",
    "content = 'a cat with a hat.'\n",
    "target_resolution = (512,512)\n",
    "resolution_tag = inference_engine.get_resolution_tag_from_resolution(target_resolution)\n",
    "\n",
    "prompt = inference_engine.default_generation_template.format(resolution_tag=resolution_tag, content=content)\n",
    "unconditional_prompt = inference_engine.default_generation_unconditional_template.format(resolution_tag=resolution_tag)\n",
    "\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"unconditional prompt: {unconditional_prompt}\")\n",
    "\n",
    "inference_config = inference_engine.prepare_inference_config(\n",
    "    temperature=1.0,\n",
    "    top_k=128,\n",
    "    top_p=1.0,\n",
    "\n",
    "    llm_cfg_scale = 2.0,\n",
    "    image_semantic_temperature= 1.0,\n",
    "    image_semantic_top_k = 2024,\n",
    "    image_semantic_top_p = 1.0,\n",
    "    \n",
    "    resolution = target_resolution,\n",
    "    unconditional_prompt = unconditional_prompt,\n",
    ")\n",
    "\n",
    "batch_data = [dict(prompt=prompt)]\n",
    "outputs = inference_engine.inference_mllm(batch_data, inference_config, is_img_gen_task=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76b69703",
   "metadata": {},
   "outputs": [],
   "source": [
    "inference_config = inference_engine.prepare_inference_config(\n",
    "    temperature=1.0,\n",
    "    top_k=128,\n",
    "    top_p=1.0,\n",
    "\n",
    "    llm_cfg_scale = 2.0,\n",
    "    image_semantic_temperature= 1.0,\n",
    "    image_semantic_top_k = 2024,\n",
    "    image_semantic_top_p = 1.0,\n",
    "    \n",
    "    diffusion_cfg_scale=1.5,\n",
    "    diffusion_num_inference_steps=50,\n",
    "\n",
    "    resolution = target_resolution,\n",
    "    unconditional_prompt = unconditional_prompt,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed5626c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using vq tokenizer to decode image.\n",
    "out_images = inference_engine.inference_tokenizer_decoder(outputs, inference_config, use_diffusion_decoder=False)\n",
    "print(f\"Image prompt: {content}\")\n",
    "generated_image = convert_np_to_pil_img(out_images, outputs)[0]\n",
    "print(f'Generated Image Size: {generated_image.size}')\n",
    "generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "595d3ba5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using sdxl diffusion decoder to decode image.\n",
    "out_images = inference_engine.inference_tokenizer_decoder(outputs, inference_config, use_diffusion_decoder=True)\n",
    "generated_image = convert_np_to_pil_img(out_images, outputs)[0]\n",
    "print(f'Generated Image Size: {generated_image.size}')\n",
    "generated_image"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9d8e01",
   "metadata": {},
   "source": [
    "## 6. Image Editing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fddfb72",
   "metadata": {},
   "outputs": [],
   "source": [
    "instruction = 'Change the color of the boots to a deep forest green.'\n",
    "image_path = '../configs/data_configs/test_data_examples/EditingSingleTurnExample/images/0.jpg'\n",
    "\n",
    "prompt = inference_engine.default_editing_template.format(resolution_tag='', content=instruction)\n",
    "unconditional_prompt = inference_engine.default_editing_unconditional_template.format(resolution_tag='')\n",
    "print(f\"prompt: {prompt}\")\n",
    "print(f\"unconditional prompt: {unconditional_prompt}\")\n",
    "\n",
    "input_image = Image.open(image_path).convert('RGB')\n",
    "original_image_size = input_image.size\n",
    "show_image(input_image)\n",
    "inference_config = inference_engine.prepare_inference_config(\n",
    "    temperature=1.0,\n",
    "    top_k=128,\n",
    "    top_p=1.0,\n",
    "    \n",
    "    llm_cfg_scale = 1.5,\n",
    "    diffusion_cfg_scale=1.5,\n",
    "    diffusion_num_inference_steps=50,\n",
    "    \n",
    "    image_semantic_temperature= 0.7,\n",
    "    image_semantic_top_k = 512,\n",
    "    image_semantic_top_p = 0.8,\n",
    "    unconditional_prompt=unconditional_prompt,\n",
    "    #    resolution=(512,512)  # the resolution will be obtrain from the source image within the code.\n",
    ")\n",
    "\n",
    "batch_data = [\n",
    "    dict(prompt=prompt, images_data=[input_image])\n",
    "]\n",
    "\n",
    "outputs = inference_engine.inference_mllm(batch_data, inference_config, is_img_gen_task=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57deb6d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  using vq tokenizer to decode image.\n",
    "out_images = inference_engine.inference_tokenizer_decoder(outputs, inference_config, use_diffusion_decoder=False)\n",
    "padded_image = convert_np_to_pil_img(out_images, outputs)[0]\n",
    "generated_image = inference_engine.unpad_and_resize_back(padded_image, outputs[0]['original_sizes'][0], outputs[0]['original_sizes'][1])\n",
    "generated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d73c7a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Using sdxl diffusion decoder to decode image.\n",
    "out_images = inference_engine.inference_tokenizer_decoder(outputs, inference_config, use_diffusion_decoder=True)\n",
    "padded_image= convert_np_to_pil_img(out_images, outputs)[0]\n",
    "generated_image = inference_engine.unpad_and_resize_back(padded_image, outputs[0]['original_sizes'][0], outputs[0]['original_sizes'][1])\n",
    "generated_image"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}